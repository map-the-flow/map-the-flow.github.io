<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="This paper presents a systematic analysis of where and how information flows in VideoLLMs for temporal reasoning in VideoQA, revealing key patterns and effective pathways.">
  <meta name="keywords" content="Video Large Language Models, Information Flow Analysis, Video Question Answering">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--  <div class="navbar-brand">-->
<!--    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--    </a>-->
<!--  </div>-->
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://keunhong.com">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->
<!--    </div>-->
<!--  </div>-->
<!--</nav>-->


<!--<section class="hero">-->
<!--  <div class="hero-body">-->
<!--    <div class="container is-max-desktop">-->
<!--      <div class="columns is-centered">-->
<!--        <div class="column has-text-centered">-->
<!--          <h1 class="title is-1 publication-title">Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs</h1>-->
<!--          <div class="is-size-5 publication-authors">-->
<!--            <span class="author-block">-->
<!--              <a href="https://byminji.github.io/">Minji Kim</a><sup>1*</sup>,</span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://scholar.google.co.kr/citations?user=u-9bdkwAAAAJ&hl=en">Taekyung Kim</a><sup>2*</sup>,</span>-->
<!--            <span class="author-block">-->
<!--              <a href="https://cv.snu.ac.kr/index.php/~bhhan/">Bohyung Han</a><sup>1</sup>-->
<!--            </span>-->
<!--          </div>-->

<!--          <div class="is-size-5 publication-authors">-->
<!--            <span class="author-block"><sup>1</sup>Seoul National University</span>-->
<!--            <span class="author-block"><sup>2</sup>NAVER AI Lab</span>-->
<!--          </div>-->

<!--          <div class="is-size-6 publication-authors" style="margin-top: 0.5rem;">-->
<!--            <span class="author-block"><sup>*</sup>These authors contributed equally to this work.</span>-->
<!--          </div>-->

<!--          <div class="column has-text-centered">-->
<!--            <div class="publication-links">-->
<!--              &lt;!&ndash; PDF Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->
<!--&lt;!&ndash;              &lt;!&ndash; Video Link. &ndash;&gt;&ndash;&gt;-->
<!--&lt;!&ndash;              <span class="link-block">&ndash;&gt;-->
<!--&lt;!&ndash;                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"&ndash;&gt;-->
<!--&lt;!&ndash;                   class="external-link button is-normal is-rounded is-dark">&ndash;&gt;-->
<!--&lt;!&ndash;                  <span class="icon">&ndash;&gt;-->
<!--&lt;!&ndash;                      <i class="fab fa-youtube"></i>&ndash;&gt;-->
<!--&lt;!&ndash;                  </span>&ndash;&gt;-->
<!--&lt;!&ndash;                  <span>Video</span>&ndash;&gt;-->
<!--&lt;!&ndash;                </a>&ndash;&gt;-->
<!--&lt;!&ndash;              </span>&ndash;&gt;-->
<!--              &lt;!&ndash; Code Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/byminji/map-the-flow"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                  </span>-->
<!--                  <span>Code</span>-->
<!--                  </a>-->
<!--              </span>-->
<!--&lt;!&ndash;              &lt;!&ndash; Dataset Link. &ndash;&gt;&ndash;&gt;-->
<!--&lt;!&ndash;              <span class="link-block">&ndash;&gt;-->
<!--&lt;!&ndash;                <a href="https://github.com/google/nerfies/releases/tag/0.1"&ndash;&gt;-->
<!--&lt;!&ndash;                   class="external-link button is-normal is-rounded is-dark">&ndash;&gt;-->
<!--&lt;!&ndash;                  <span class="icon">&ndash;&gt;-->
<!--&lt;!&ndash;                      <i class="far fa-images"></i>&ndash;&gt;-->
<!--&lt;!&ndash;                  </span>&ndash;&gt;-->
<!--&lt;!&ndash;                  <span>Data</span>&ndash;&gt;-->
<!--&lt;!&ndash;                  </a>&ndash;&gt;-->
<!--            </div>-->

<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->





<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs
          </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://byminji.github.io/">Minji Kim</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.kr/citations?user=u-9bdkwAAAAJ&hl=en">Taekyung Kim</a><sup>2*</sup>,
            </span>
            <span class="author-block">
              <a href="https://cv.snu.ac.kr/index.php/~bhhan/">Bohyung Han</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Seoul National University</span>
            <span class="author-block"><sup>2</sup>NAVER AI Lab</span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 0.5rem;">
            <span class="author-block"><sup>*</sup>These authors contributed equally to this work.</span>
          </div>

          <!-- Publication links -->
          <div class="buttons is-centered" style="margin-top: 1rem; margin-bottom: 0;">
            <a href="" class="button is-normal is-rounded is-dark">
              <span class="icon"><i class="fas fa-file-pdf"></i></span>
              <span>Paper</span>
            </a>

            <a href="" class="button is-normal is-rounded is-dark">
              <span class="icon"><i class="ai ai-arxiv"></i></span>
              <span>arXiv</span>
            </a>

            <a href="https://github.com/byminji/map-the-flow" class="button is-normal is-rounded is-dark">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>Code</span>
            </a>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- TL;DR Section -->
<section class="section" style="padding-top:1rem; padding-bottom:1rem;">
  <div class="container" style="max-width:800px; margin:0 auto;">
    <div class="content has-text-centered" style="font-weight:bold;">
      TL;DR: This paper presents a systematic analysis of where and how information flows in VideoLLMs for temporal reasoning in VideoQA, revealing key patterns and effective pathways.
    </div>
  </div>
</section>

<!-- Main Content / Teaser Section -->
<section class="section" style="padding-top:2rem; padding-bottom:2rem;">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered" style="padding-top:0;">

      <!-- Figure / Image -->
      <img src="./static/images/teaser.jpg" alt="VideoLLMs findings" style="width:100%; max-width:100%; margin-bottom:1em;" />

      <!-- Caption -->
      <div class="content has-text-left">
        <p><strong>Summary of our findings on VideoLLMs' information flow.</strong></p>
        <p>
          <strong>(a)</strong> Temporal reasoning begins with cross-frame interactions within video tokens at early-middle layers
          <span style="color:#4DAF4A;">[green]</span>, followed by video-language integration into temporal keywords in the question
          <span style="color:#984EA3;">[purple]</span>. This information is conveyed to the last token at middle-late layers
          <span style="color:#FF7F00;">[orange]</span>, where answer generation occurs
          <span style="color:#FFB302;">[yellow]</span>.
        </p>
        <p>
          <strong>(b)</strong> These effective pathways are identified via Attention Knockout, which disconnects attention pairs and
          tracks the drop in probability of the final answer to quantify their impact.
        </p>
        <p>
          <strong>(c)</strong> Layer-wise answer probability rises immediately after video-language integration, indicating that
          the model is ready to predict correct answers after the middle layers.
        </p>
        <p>
          Based on our analysis, we show that VideoLLMs can retain their VideoQA performance by selecting effective information pathways
          while suppressing a substantial amount of attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT.
        </p>
      </div>
    </div>
  </div>
</section>





<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            Video Large Language Models (VideoLLMs) extend the capabilities of vision-language models
            to spatiotemporal inputs, enabling tasks such as video question answering (VideoQA). Despite
            recent advances in VideoLLMs, their internal mechanisms on <i>where</i> and <i>how</i> they
            extract and propagate video and textual information remain less explored. In this study, we
            investigate the internal information flow of VideoLLMs using mechanistic interpretability
            techniques. Our analysis reveals consistent patterns across diverse VideoQA tasks: (1)
            <i>temporal reasoning</i> in VideoLLMs initiates with active <i>cross-frame interactions</i>
            in early-to-middle layers, (2) followed by progressive <i>video-language integration</i> in
            middle layers. This is facilitated by alignment between video representations and linguistic
            embeddings containing <i>temporal concepts</i>. (3) Upon completion of this integration, the
            model is ready to generate correct answers in middle-to-late layers. (4) Based on our
            analysis, we show that VideoLLMs can retain their VideoQA performance by selecting these
            effective information pathways while suppressing a substantial amount of attention edges,
            e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a blueprint on how VideoLLMs
            perform temporal reasoning and offer practical insights for improving model interpretability
            and downstream generalization.
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <!-- Research Question -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Research Questions</h2>
        <div class="content has-text-justified">
          <p>
            In this study, we aim to provide a <i>complete blueprint</i> that reveals the systematic behaviors of VideoLLMs on temporal reasoning tasks, with a focus on the information flow across different layers and modalities.
          </p>
          <p>
            To understand how VideoLLMs generate an <i>answer</i> from a given (<i>video</i>, <i>question</i>) pair, we decompose the temporal reasoning process into several stages and investigate the following key questions:
          </p>
          <ol>
            <li>How do VideoLLMs encode spatiotemporal information from the given flattened sequence of video tokens?</li>
            <li>How are the temporal concepts in the question extracted from video tokens and propagated to text tokens?</li>
            <li>At what stage does the model become ready to generate an answer?</li>
            <li>Can we identify effective information flow pathways sufficient to solve VideoQA?</li>
          </ol>
        </div>
      </div>
    </div>
    <!--/ Research Question -->

  </div>
</section>



<section class="section" style="padding-top:1.5rem;">
  <div class="container is-max-desktop">

    <!-- Our Findings -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-bottom:2rem;">Our Findings</h2>

        <!-- Active Temporal Interaction -->
        <h3 class="title is-4" style="margin-bottom:3rem;">1. Active Temporal Interaction Within Video Tokens in Early-to-Middle Layers</h3>
<!--        <div class="content has-text-justified" style="margin-bottom:2rem;">-->
<!--          <p>-->
<!--            Temporal reasoning begins by building spatiotemporal representations from video tokens through focused cross-frame attention in early-to-middle layers.-->
<!--            Our analysis using Attention Knockout [Geva et al., 2023], which selectively disconnects attention edges to quantify their impact,-->
<!--            shows this capability is uniquely acquired through VideoQA instruction tuning from base ImageLLMs.-->
<!--          </p>-->
<!--        </div>-->

        <!-- Figure2 -->
        <div class="content has-text-centered" style="margin-top:1em; margin-bottom:3rem;">
          <img src="./static/images/fig2_cross_frame.png" alt="Cross-frame attention impact" style="width:100%; max-width:90%;">
          <p class="has-text-justified" style="margin-top:1em;">
            <strong>Training with VideoQA data boosts cross-frame interactions in the early-to-middle layers.</strong>
            We use Attention Knockout to selectively disconnects attention edges to quantify their impact.
            Blocking cross-frame interactions in early-to-middle layers significantly harms LLaVA-NeXT-7B-Video-FT’s prediction, while LLaVA-NeXT-7B remains mostly unaffected,
            showing that this capability is uniquely acquired through VideoQA instruction tuning from base ImageLLMs.
          </p>
        </div>

        <!-- Table2 -->
        <div class="content has-text-centered" style="margin-top:1em; margin-bottom:3rem;">
          <img src="./static/images/tab2_cross_frame_impact.png" alt="Cross-frame attention impact" style="width:100%; max-width:90%;">
          <p class="has-text-justified" style="margin-top:1em;">
            <strong>Impact of cross-frame attention on answer generation.</strong> We block cross-frame attention in the first half of the total layers and measure the resulting accuracy drop. Without cross-frame attention, the model generates incorrect or even opposite answers to the given videos. This suggests that VideoLLMs rely heavily on cross-frame interactions in the early
stage to reason about temporal events.
          </p>
        </div>

        <!-- Video-Language Integration -->
        <h3 class="title is-4" style="margin-top:5rem; margin-bottom:3rem;">2. Video-Language Integration on Temporal Keywords in Middle Layers</h3>
<!--        <div class="content has-text-justified" style="margin-bottom:2rem;">-->
<!--          <p>-->
<!--            Analyzing semantic concepts in video tokens through Logit Lens [Nostalgebraist, 2020] shows that temporal concepts are emergent among video tokens in the vocabulary space.-->
<!--            Alignment between these representations and temporal keyword embeddings facilitates selective video-language integration over relevant question tokens in early-to-middle layers,-->
<!--            which is followed by information converging to the last position token in middle-to-late layers.-->
<!--          </p>-->
<!--        </div>-->

        <!-- Figure3 -->
        <div class="content has-text-centered" style="margin-top:1em; margin-bottom:3rem;">
          <img src="./static/images/fig3_overall_cross_modal_flow.png" alt="Overall cross-modal flow" style="width:100%; max-width:90%;">
          <p class="has-text-justified" style="margin-top:1em;">
            <strong>Overall cross-modal information flow in VideoLLMs.</strong>
            We analyze changes in the prediction probability when intervening on attention edges between video, question,
            and last token (i.e., the starting position for answer generation).
            Information from the video tokens is conveyed to the question tokens in the early-to-middle layers, followed
            by the transfer of information from the question tokens to the last token in the middle-to-late layers.
          </p>
        </div>

        <!-- Figure4 -->
        <div class="content has-text-centered" style="margin-top:1em; margin-bottom:3rem;">
          <img src="./static/images/fig4_logit_lens.png" alt="Emergence of temporal concepts" style="width:100%; max-width:90%;">
          <p class="has-text-justified" style="margin-top:1em;">
            <strong>Emergence of temporal concepts in video tokens.</strong>
            Analyzing semantic concepts in video tokens through Logit Lens shows that
            temporal concepts are emergent among video tokens in the vocabulary space.
            Interestingly, spatial concepts start to appear in the very early layers, whereas temporal concepts develop later in the middle layers.
          </p>
        </div>

        <!-- Figure5 -->
        <div class="content has-text-centered" style="margin-top:1em; margin-bottom:3rem;">
          <img src="./static/images/fig5_video-to-question_attention.png" alt="Video-to-question attention" style="width:100%; max-width:90%;">
          <p class="has-text-justified" style="margin-top:1em;">
            <strong>Video-language alignment enables selective spatiotemporal propagation.</strong>
            In this video-to-question attention maps,
            (a) with spatiotemporal interactions, each question token attends to semantically relevant regions:
            "begins" focuses on blue sphere at start, "ends" on blue sphere and green square at end.
            (b) When temporal interactions among video tokens are blocked, video-text alignment fails and text tokens
            instead attend to positionally proximate regions rather than semantically relevant ones.
          </p>
        </div>

        <!-- Answer Generation -->
        <h3 class="title is-4" style="margin-top:5rem; margin-bottom:3rem;">3. Answer Generation at Middle-to-Late Layers</h3>
        <!-- Figure8 -->
        <div class="content has-text-centered" style="margin-top:1em; margin-bottom:3rem;">
          <img src="./static/images/fig8_answer_generation.png" alt="Answer generation" style="width:100%; max-width:90%;">
          <p class="has-text-justified" style="margin-top:1em;">
            Tracing layer-wise answer probability at the last token reveals that the model is prepared to generate a correct answer immediately once the video-language integration concludes after middle layers.
          </p>
        </div>

        <!-- Effective Information Flow -->
        <h3 class="title is-4" style="margin-top:5rem; margin-bottom:3rem;">4. Effective Information Flow Pathways Are Sufficient for Solving VideoQA Tasks</h3>
        <!-- Tab3 -->
        <div class="content has-text-centered" style="margin-top:1em; margin-bottom:3rem;">
          <img src="./static/images/tab3_information_flow_pathways.png" alt="Effective information flow pathways" style="width:100%; max-width:90%;">
          <p class="has-text-justified" style="margin-top:1em;">
            To validate the above findings, we disable all information pathways except those identified as critical.
            Evaluation on VideoQA benchmarks shows that the models retain performance comparable to baselines,
            demonstrating that these effective pathways suffice for accurate answer generation.
          </p>
        </div>

      </div>
    </div>
    <!--/ Our Findings -->

  </div>
</section>



<!--<section class="section" id="BibTeX">-->
<!--  <div class="container is-max-desktop content">-->
<!--    <h2 class="title">BibTeX</h2>-->
<!--    <pre><code>@article{park2021nerfies,-->
<!--  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},-->
<!--  title     = {Nerfies: Deformable Neural Radiance Fields},-->
<!--  journal   = {ICCV},-->
<!--  year      = {2021},-->
<!--}</code></pre>-->
<!--  </div>-->
<!--</section>-->


<footer class="footer">
  <div class="container">
<!--    <div class="content has-text-centered">-->
<!--      <a class="icon-link"-->
<!--         href="./static/videos/nerfies_paper.pdf">-->
<!--        <i class="fas fa-file-pdf"></i>-->
<!--      </a>-->
<!--      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>-->
<!--        <i class="fab fa-github"></i>-->
<!--      </a>-->
<!--    </div>-->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a> and licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
<!--          <p>-->
<!--            This means you are free to borrow the <a-->
<!--              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,-->
<!--            we just ask that you link back to this page in the footer.-->
<!--            Please remember to remove the analytics code included in the header of the website which-->
<!--            you do not want on your website.-->
<!--          </p>-->
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
